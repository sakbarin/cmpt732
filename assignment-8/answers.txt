1) What did you see in the execution plan for the “join in Spark” solution? Why was the execution so fast (and the memory usage so small)? 

- This is because of applying PushedFilter. We applied a filter on dataframe to get a bunch of records with specific IDs. This filter is pushed down to database to reduce the amount of data passed between database and Spark. This leads to the filtering of unwanted records and keeping only a small number of records required for operation. So, Spark performs joins using only a small portion of that 39M records which will be fast as a result.

ObjectHashAggregate(keys=[orderkey#0, totalprice#8], functions=[collect_set(name#55, 0, 0)])
+- ObjectHashAggregate(keys=[orderkey#0, totalprice#8], functions=[partial_collect_set(name#55, 0, 0)])
   +- *(4) Sort [orderkey#0 ASC NULLS FIRST], true, 0
      +- Exchange rangepartitioning(orderkey#0 ASC NULLS FIRST, 200)
         +- *(3) Project [orderkey#0, totalprice#8, name#55]
            +- *(3) BroadcastHashJoin [partkey#25], [partkey#50], Inner, BuildRight
               :- *(3) Project [orderkey#0, totalprice#8, partkey#25]
               :  +- *(3) BroadcastHashJoin [orderkey#0], [orderkey#19], Inner, BuildLeft
               :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
               :     :  +- *(1) Filter (cast(orderkey#0 as string) IN (151201,986499,28710,193734,810689) && isnotnull(orderkey#0))
               :     :     +- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@77815665 [orderkey#0,totalprice#8] PushedFilters: [IsNotNull(orderkey)], ReadSchema: struct<orderkey:int,totalprice:decimal(38,18)>
               :     +- *(3) Filter ((cast(orderkey#19 as string) IN (151201,986499,28710,193734,810689) && isnotnull(orderkey#19)) && isnotnull(partkey#25))
               :        +- *(3) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@428c8499 [orderkey#19,partkey#25] PushedFilters: [IsNotNull(orderkey), IsNotNull(partkey)], ReadSchema: struct<orderkey:int,partkey:int>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
                  +- *(2) Filter isnotnull(partkey#50)
                     +- *(2) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@6f483d7e [partkey#50,name#55] PushedFilters: [IsNotNull(partkey)], ReadSchema: struct<partkey:int,name:string>

2) What was the CREATE TABLE statement you used for the orders_parts table? 

- Follwing is the script used to create "orders_parts" table: 

CREATE TABLE orders_parts (
    orderkey int,
    custkey int,
    orderstatus text,
    totalprice decimal,
    orderdate date,
    order_priority text,
    clerk text,
    ship_priority int,
    comment text,
    part_names set<text>,
    PRIMARY KEY (orderkey)
);

3) What were the running times of the two tpch_orders_* programs on the tpch2 data on the cluster? These orderkeys have results in that data set: 2579142 2816486 586119 441985 2863331. 

- tpch_orders_denorm program was [30 seconds] faster.

- The running time using tpch_orders_denorm was:
real	0m49.403s
user	0m27.456s
sys	0m2.216s

- The running time using tpch_orders_df was: 
real	1m19.472s
user	0m42.932s
sys	0m3.304s

4) Consider the logic that you would have to implement to maintain the denormalized data (assuming that the orders table had the part_names column in the main data set). Write a few sentences on what you'd have to do when inserting/updating/deleting data in this case. 

- When creating a new order, the program should prepare set<part> for the new order [using order, lineitem, part tables] and then insert a new record holding all columns of order and set<part> to orders_parts table. We should also ensure that part_names column has a value and it's not NULL.

- The problem arises when updating the name of a part. In this case, we have to find all records in orders table which has that specific part [using order, lineitem, part table]. Then, we should get records with the same orderkeys from orders_parts table. Then, find the name of target part in each record and update it to the new value in each record.

- Another problem arises when deleting a part. In this scenario, the same as update scenario, we should again find all records from orders table which has that part name [using order, lineitem, part table]. Then find equivalent records from orders_parts and delete the part name from set<parts> of each record. And update this column value for each record.
