1) What happened to the HDFS file when one of the nodes it was stored on failed?

- After failing one datanode, in the HDFS web frontend under DataNodes tab, number of active datanodes changed from 4 to 3. The failed datanode (hdfs-datanode-3) row turned red and showed the last contact date and time. First overview page showed number of under replicated blocks equal to 15 for some seconds, then it changed again to 0. This shows that blocks of the files which were stored on hdfs-datanode-3 has been copied to other datanodes again after failure.

2) How did YARN/MapReduce/Spark behave when one of the compute nodes disappeared?

- Initially, the job is handle using three executors. Calculating sc.range(10000000000, numSlices=100).sum() started to work as normal. In the middle of the process, as I stopped nodemanager-3 some errors appeared in Spark console as following showing that one nodemanager has failed to work. The ResourceManager knows what task each executor is running. So, by failure of a nodemanager, ResouceManager distributes it's remaining tasks to other executors. And the process continues up to the end (in this example to show the sum) without any problem. This shows the robustness of hadoop. Although the time of process increases in this case, it prevents the process from failure.

Exceptions:

2019-11-12 03:50:04,224 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container marked as failed: container_1573530320266_0001_01_000004 on host: yarn-nodemanager-3. Exit status: -100. Diagnostics: Container released on a *lost* node.

2019-11-12 03:50:04,225 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container marked as failed: container_1573530320266_0001_01_000006 on host: yarn-nodemanager-3. Exit status: -100. Diagnostics: Container released on a *lost* node.

2019-11-12 03:50:04,229 ERROR cluster.YarnScheduler: Lost executor 3 on yarn-nodemanager-3: Container marked as failed: container_1573530320266_0001_01_000004 on host: yarn-nodemanager-3. Exit status: -100. Diagnostics: Container released on a *lost* node.

2019-11-12 03:50:04,283 ERROR cluster.YarnScheduler: Lost executor 5 on yarn-nodemanager-3: Container marked as failed: container_1573530320266_0001_01_000006 on host: yarn-nodemanager-3. Exit status: -100. Diagnostics: Container released on a *lost* node.

3) Were there more things you'd like to try with this cluster, or anything you did try that you think should have been in this assignment? 

- N/A
