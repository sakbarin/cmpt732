1) What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

- The size of files in wordcount-5 data set varies significantly (Max: 250MB, Min: 80KB). Therefore, one unlucky executor stucks in processing that large partition while others have finished processing their small partitions. 

- Although repartition() adds overhead and additional seconds to the whole timing, it's worth applying in this case since it makes the partition sizes even and reduces the time required to finish the process significantly. Timing reduced by 1 minute after applying repartition() method.

- without repartition:
real	3m54.642s
user	0m23.960s
sys	0m01.672s

- with repartition:
real	2m47.545s
user	0m21.912s
sys	0m01.848s

2) The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

- File sizes are distributed evenly in wordcount-3 data set and we will have evenly distributed partitions. So, if we apply repartition(), we should expect the computation to take longer (a few seconds) because of extra computations and overhead we put on the computer.

3) How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)

- One solution to get better timing is to split large files into some smaller files using Linux "split" command. So that we will have files sizes in a specific range (neither very large nor very small).

4) When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?

- The best range that worked on my computer was something between 8 and 64. Numbers smaller than 8 and greater than 64, lead to a longer time for the calculations to finish.

5) How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation? 

- multi-thread Euler - Python:
real	5m24.687s
user	0m20.680s
sys	0m01.029s

- multi-thread Euler - PyPy:
real	1m20.980s
user	0m18.806s
sys	0m00.974s

- Multi-threaded Euler using Python took 4 minutes longer than PyPy in this example which is a considerable amount.

- single-threaded Euler - PyPy:
real	1m42.687s
user	1m41.957s
sys	0m00.048s

- Single-threaded Euler (PyPy) is surprisingly much faster than multi-threaded Euler (Python).
- Single-threaded Euler (PyPy) took 20 seconds longer than multi-threaded Euler (PyPy) which is not a significant amount.

- These are the timings for running single-threaded Euler using c:
real	0m58.671s
user	0m58.648s
sys	0m00.000s
